\section{Model Training}\label{sec3.model_training}




\subsection{Model Architecture}\label{sec3.1}

We opt to extend the Mistral model architecture~\cite{jiang2023mistral7b} due to its ability to achieve high performance while maintaining efficient inference speeds. The original Mistral 7B model demonstrates superior performance compared to multiple 7B language models and even outperforms larger models on various evaluation benchmarks. Notably, it surpasses the LLaMA 34B model~\cite{roziere2023code} in tasks such as mathematics and code generation.

 

\begin{wraptable}{r}{4cm}
\vspace{-12pt}\caption{Parameter setting.}\label{wrap-tab:1}
%\begin{table}[]
\begin{tabular}{l|l}
\hline
Parameter    & Value \\ \hline
n\_layers    & 36    \\
dim          & 4096  \\
head\_dim    & 128   \\
hidden\_dim  & 14336 \\
n\_heads     & 32    \\
n\_kv\_heads & 8     \\
 \hline
\end{tabular}
%\end{table}
\end{wraptable} 
The original Mistral model leverages grouped-query attention (GQA)\cite{ainslie2023gqa} and sliding window attention (SWA)\cite{beltagy2020longformer}. GQA reduces memory requirements during decoding, allowing for larger batch sizes and higher throughput, and it significantly accelerates inference speedâ€”an essential factor in real-time applications. Meanwhile, SWA effectively handles long sequences without incurring substantial computational overhead. By incorporating these techniques, the model achieves significant improvements in performance and efficiency, which we have adopted in our extended model.



Building upon the original Mistral model, which consists of 32 blocks, we have extended the architecture to 36 blocks. Furthermore, we also employ GQA to partition the query heads into multiple groups, each sharing a single key head and value head. This approach interpolates between multi-query attention (MQA) and multi-head attention (MHA) in large language models, striking a balance between the computational speed of MQA and the representational quality of MHA, thereby providing a favorable trade-off. Additionally, our model incorporates a rolling buffer cache with a fixed attention span, effectively limiting cache size and preventing excessive memory usage when processing long sequences.




\subsection{Training Data}\label{sec3.3}

Data are fundamental to the pre-training of LLMs. Preparing such training data requires careful consideration of multiple challenges, including handling sensitive information, ensuring comprehensive knowledge coverage, and achieving higher efficiency with improved data quality. 

In this section, we detail the processes of preparing textual data from general domains and coding data related to programming languages.




\subsubsection{Text Data}

We use a mix of data from SlimPajama \cite{cerebras2023slimpajama} and DCLM-BASELINE \cite{li2024datacomp} as our text training data.

During the training of LLaMA, it was demonstrated that the performance of a 7B model continues to improve even after being trained on more than 1T tokens \cite{touvron2023llama}. Given the outstanding performance of LLaMA, its data collection methodology was rapidly replicated, leading to the release of RedPajama, an open-source dataset containing 1.2 trillion tokens \cite{weber2024redpajama}.


However, subsequent analyses reveal a significant limitation: some corpora within RedPajama contain a large percentage of duplicate content. The deduplication guidelines in RedPajama operate only within individual data sources, leaving inter-source duplicates largely unaddressed. To improve data quality and training efficiency, SlimPajama\footnote{https://huggingface.co/datasets/cerebras/SlimPajama-627B} was developed as a refined iteration of RedPajama, offering a cleaned and extensively deduplicated version \cite{cerebras2023slimpajama}.



SlimPajama implements a rigorous two-stage preprocessing pipeline to enhance data quality. In the first stage, short and low-quality documents are removed from RedPajama. Specifically, documents that have fewer than 200 characters after removing punctuation, space symbols, newlines, and tabs are filtered out, as these documents typically contain only metadata and lack useful information. As a result of this step, 1.86\% of RedPajama documents are eliminated.



The second step involves removing duplicate data, as deduplication enhances training efficiency and reduces memorization, thereby decreasing the likelihood of generating text solely by recalling training data \cite{penedo2023refinedweb, abbas2023semdedup, face2023, lee2021deduplicating, holtzman2019curious}. To perform deduplication, document signatures are created using pre-processed, lower-cased 13-grams. Subsequently, MinHashLSH~\cite{leskovec2014mining} is employed to identify and eliminate duplicates based on a Jaccard similarity threshold of 0.8. Deduplication is performed both within and across data sources. Overall, by pruning 49.6\% of the bytes from the RedPajama dataset, the 627B-token SlimPajama dataset is obtained.




Additionally, we utilize the DCLM-BASELINE \cite{li2024datacomp} dataset\footnote{https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0}, which is derived from CommonCrawl, a web-crawled dataset \cite{patel2020introduction}. The construction of DCLM-BASELINE involves several steps. First, resiliparse is employed to extract text from CommonCrawl. Second, deduplication is performed using MinHash~\cite{broder1997resemblance} within a suffix array pipeline \cite{fineweb2024, lee2021deduplicating} and near-duplicate Bloom filtering, which enhances the exact document and paragraph deduplication scheme \cite{soldaini2024dolma}. Third, recent studies \cite{brandfonbrener2024color, soldaini2024dolma, fang2023data} demonstrate that utilizing learnable models as quality filters leads to downstream performance improvements. Consequently, DCLM-BASELINE applies a fastText OH-2.5 combined with an ELI5 classifier score to retain the top 10\% of documents.



\subsubsection{Coding Data}

Programming is crucial for LLMs to support various downstream tasks, such as code completion from natural language descriptions, documentation generation for individual functions, and auto-completion of code snippets. Furthermore, as code is generally better structured and organized than natural language, training on code data may improve the LLM reasoning capabilities \cite{groeneveld2024olmo}. Therefore, We use part of the-stack-dedup \cite{kocetkov2022stack} dataset\footnote{https://huggingface.co/datasets/bigcode/the-stack-dedup} during the pretraining.

The Stack comprises more than 6TB of permissively-licensed source code files across 358 programming languages~\cite{kocetkov2022stack}. This carefully curated resource was designed to enhance the code generation capabilities of LLMs. It facilitates the synthesis of programs by code-generating AI systems from both natural language descriptions and existing code snippets.


To construct the Stack dataset, 220.92 million active GitHub repositories were collected from event archives published between 2015 and 2022 on GHArchive. Of these repositories, only 137.36 million were publicly accessible on GitHub, resulting in 51.76 billion downloaded files. After initial filtering, 5.28 billion unique files were identified, with an uncompressed size of 92.36 TB.



To ensure data quality, near-deduplication was implemented within the preprocessing pipeline in addition to exact deduplication. Specifically, MinHash with 256 permutations was computed for all documents, and Locality Sensitive Hashing was employed to identify clusters of duplicates. Within these clusters, Jaccard similarities were calculated to detect near-duplicates using a similarity threshold of 0.85. Approximately 40\% of permissively licensed files were identified as (near-)duplicates and subsequently removed.




\subsubsection{Capability Enhancement}

LLMs are expected to demonstrate capabilities such as reasoning, mathematical problem-solving, and knowledge memorizing. However, a significant challenge lies in that, in the pre-training process, high-quality capability-related data is sparsely distributed in the entire corpus, and thereby it is difficult for models to be proficient at these above-mentioned capabilities. Previous research, such as work on Qwen~\cite{bai2023qwen}, GLM-130B~\cite{zeng2023glmb}, Nemotron-4~\cite{parmar2024nemotron}, has tried to incorporate instruction-based or high-quality data during the pre-training stage to enhance these abilities. In our study, we collect open-source data from HuggingFace, primarily utilizing the training datasets of various evaluation benchmarks such as MMLU \cite{hendrycks2021measuringmassivemultitasklanguage} and HellaSwag \cite{zellers2019hellaswag}. These data are used experimentally to investigate the relationship between high-quality, capability-focused training data and model performance.


\subsection{Training Configuration}

The total number of tokens used for pre-training our Moxin-7B model is over 2T, and the pre-training process consists of three phases. In the first phase, we use pre-training corpora with the context length of 2k. In the second phase, we use pre-training corpora with the context length of 4k. In the third phase, we utilize the capability-specific enhancement data. We provide the model performance with only the first two phases and also with all three phases to validate the performance of the third phase. 

We use Colossal-AI \cite{li2023colossal} as our training framework. Colossal-AI is a unified deep learning system that provides the fullest set of acceleration techniques for the AI community. With its modular design, ColossalAI allows for a free combination of these techniques to achieve the best training speedup. Colossal-AI's optimized parallelism and heterogeneous training methods are employed to achieve superior system performance compared to baseline systems. These methods are provided through user-friendly APIs, requiring minimal code modifications.

  
During training, AdamW \cite{loshchilov2017decoupled} with $\beta_1 = 0.9$, $\beta_2 = 0.95$, $\epsilon = 1e^{-8}$ and weight decay = 0.1 is used to optimize the model. We use the cosine learning rate decay and the learning rate decays to 10\% of its maximum.  Learning Rate is set to $2e^{-6}$. 

\subsection{Alignment}


Following the pre-training phase, we fine-tune the model into a helpful and harmless AI assistant.
In our Alignment stage, we mainly use supervised fine-tuning (SFT), during which we fine-tune the model to follow diverse human instructions by high-quality instruction data. We use the Tulu v2 dataset \cite{ivison2023camels} for instruction tuning. The dataset consists of a mix of FLAN, Open Assistant 1, ShareGPT, GPT4-Alpaca, LIMA, and so on. 

\subsection{Long-Context}\label{sec3.4}

To deal with the long-context problem, our model leverages grouped-query attention (GQA)~\cite{ainslie2023gqa}, sliding window attention (SWA)~\cite{beltagy2020longformer}, and Rolling Buffer Cache \cite{jiang2023mistral7b}. GQA reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput. 

Besides, SWA can handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. SWA exploits the stacked layers of a transformer to attend information beyond the window size $W$. 
At the last layer, with SWA, using a window size of $W = 4096$, we have a theoretical attention span of approximately $14K$ tokens or above. 


Our model adopts Rolling Buffer Cache which limits the cache size using a rolling buffer cache with a fixed attention span. The cache has a fixed size of $W$, and the keys and values for the timestep $i$ are stored in position $i$ mod $W$ of the cache. As a result, when the position $i$ is larger than $W$, past values in the cache are overwritten, and the size of the cache stops increasing. On a sequence length of $32k$ tokens, this reduces the cache memory usage by 8$\times$, without impacting the model quality.

With the above techniques, our model can support $32K$ context length with fast inference and low memory cost.
