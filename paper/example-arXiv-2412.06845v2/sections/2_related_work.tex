\section{Related Work}\label{sec2.related_work}
\subsection{Models, Tokenizers, and Training}\label{sec2.1}

\textbf{Models.} State-of-the-art large language models (LLMs) typically comprise a substantial number of parameters, often approaching or exceeding 100 billion~\cite{dubey2024llama, achiam2023gpt, team2023gemini}. To facilitate broader accessibility, smaller models with fewer than 20 billion parameters, and even those around 7 billion parameters, have been developed~\cite{bai2023qwen,yang2024qwen2,dubey2024llama,jiang2023mistral7b,lieber2024jamba,team2024jamba}. In addition, efficiency-enhancing techniques, such as implementing MAMBA-based architectures in Jamba, have been employed to optimize performance~\cite{lieber2024jamba,team2024jamba}.



\textbf{Tokenizers.} Tokenizers are essential to convert raw data into a suitable format for model processing. Many contemporary models employ Byte-Pair Encoding (BPE)\cite{sennrich2015neural}, with OpenAI's \texttt{tiktoken} tokenizer\cite{tiktoken} being a notable implementation. However, for languages that handle tokens differently from Romance languages, alternatives such as SentencePiece~\cite{kudo2018sentencepiece} are utilized, as seen in XLNet~\cite{yang2019xlnet}. Hugging Face offers an excellent summary of state-of-the-art tokenizers with practical examples~\cite{huggingface_tokens}. Moreover, tokenization extends beyond text modalities; many foundational models now include multimodal capabilities, processing documents, audio, images, and even videos~\cite{reid2024gemini,maaz2023video,zhang2023video,zhang2024mm}.



\textbf{Training.} To enhance the performance of smaller models beyond their inherent limitations, various training strategies can be employed. A notable example is the application of Mixture of Experts (MoE) training, which has achieved significant success in models like Mixtral~\cite{jiang2024mixtral}.



\subsection{Data curation methods}\label{sec2.2}

Researchers commonly collect large datasets for training language models (LMs)\cite{mann2020language} by performing web crawls. However, these datasets often contain undesirable content, necessitating data curation to improve their quality. To enhance model performance\cite{penedo2023refinedweb, rae2021scaling, mann2020language, wenzek2019ccnet}, several data curation techniques are widely employed. These include filtering by language~\cite{xue2020mt5,raffel2020exploring,conneau2019cross}, heuristic-based filtering~\cite{penedo2023refinedweb, chen2021evaluating, gao2020pile}, quality filtering~\cite{sachdeva2024train, longpre2023pretrainer, du2022glam}, data deduplication~\cite{lee2021deduplicating,agarwal2009url}, and data mixing~\cite{li2024datacomp, albalak2023efficient, shen2023slimpajama}.




\subsection{Open-source datasets}\label{sec2.3}

As the scale of LMs has increased in recent years~\cite{dubey2024llama, team2024gemma, chowdhery2023palm, achiam2023gpt}, the community has correspondingly curated larger datasets to support their training. Early datasets include the C4 dataset, containing 160 billion tokens, and The Pile~\cite{gao2020pile}, which comprises 300 billion tokens. More recently, even larger datasets have been introduced: RefinedWeb~\cite{penedo2023refinedweb} with 600 billion tokens, Dolma~\cite{soldaini2024dolma} with 3 trillion tokens, FineWeb~\cite{penedo2024fineweb} with 15 trillion tokens, and RedPajama-v2~\cite{ostendorffllm} containing 30 trillion tokens. In addition to these general-purpose datasets, large domain-specific datasets have also been developed. For instance, StackV2~\cite{lozhkov2024starcoder}, a code-focused dataset, includes 900 billion tokens, and FineWeb-Edu~\cite{penedo2024fineweb}, a high-quality filtered educational text dataset, contains 1.3 trillion tokens.

