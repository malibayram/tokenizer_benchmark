\section{Evaluation}\label{sec4.evaluation}


We conducted comprehensive performance comparisons against leading language models of comparable scale, including Mistral-7B \cite{jiang2023mistral7b}, LLaMA 2-7B \cite{touvron2023llama}, Gemma-7B \cite{team2024gemma}, and Qwen v2-7B \cite{yang2024qwen2}. These models were selected based on their demonstrated excellence within the 7B or 8B category and represent diverse development approaches from various research organizations worldwide. To ensure a robust evaluation, we re-run all benchmarks with the same evaluation pipeline for fair comparisons. Specifically, we use lm-evaluation-harness \cite{lmevaluationharness} and opencompass \cite{open_compass} for evaluation.

Lm-evaluation-harness provides a unified framework to test generative language models on a large number of different evaluation tasks. It supports over 60 standard academic benchmarks for LLMs, with hundreds of subtasks and variants implemented.
This framework is versatile as it extends to models implemented through various architectures, including transformers (including quantization via AutoGPTQ~\cite{AutoGPTQ}), GPT-NeoX~\cite{black2022gpt}, and Megatron-DeepSpeed~\cite{song2023deepspeed4science}, all unified through a flexible, tokenization-agnostic interface.
The framework is reliable, as evidenced by serving as the backend for HuggingFace's popular Open LLM Leaderboard and being utilized by dozens of organizations, including NVIDIA, Cohere, BigScience, BigCode, Nous Research, and Mosaic ML. 

To complement, we also employed openCompass. This framework performs an in-depth and holistic assessment of large language models structured around eight fundamental dimensions of language model capabilities: language comprehension, knowledge precision, logical deduction, creative ideation, mathematical problem-solving, programming proficiency, extended text analysis, and intelligent agent engagement.



\subsection{Evaluation Tasks}\label{sec4.1}

We evaluate the model performance on various tasks below.


\begin{itemize}
\item AI2 Reasoning Challenge (ARC)~\cite{allenai:arc} - a set of genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set (ARC-C) and an Easy Set (ARC-E), where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.
\item HellaSwag~\cite{zellers2019hellaswag} - a test of commonsense natural language inference, which is easy for humans (~95\%) but challenging for SOTA models. It consists of 70,000 multiple-choice questions. Each question presents a scenario followed by four possible outcomes, asking the model to select the most reasonable conclusion.
\item MMLU~\cite{van2024ai} - a test to measure a text model's multitask accuracy. The test covers 57 tasks, including elementary mathematics, US history, computer science, law, etc.
\item Winogrande~\cite{sakaguchi2021winogrande} - an adversarial and difficult Winograd benchmark at scale, for commonsense reasoning. It contains 44,000 multiple-choice questions with two options each. It requires the model to choose the appropriate entity word for the pronoun in the descriptive text based on the scenario.
\item PIQA~\cite{bisk2019piqareasoningphysicalcommonsense} -   the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering (PIQA). Physical commonsense knowledge is a major challenge on the road to true AI-completeness, including robots that interact with the world and understand natural language. PIQA focuses on everyday situations with a preference for atypical solutions. 
\end{itemize}




\subsection{Evaluation Results}\label{sec4.2}


We name the initial model as Moxin-7B-original, which presents the foundation model before fine-tuning on the training data of the evaluation datasets. After subsequent partial fine-tuning of Moxin-7B-original on the training data of the evaluation datasets, we developed Moxin-7B-finetuned, enabling direct assessment of how targeted fine-tuning affects model performance. 





\subsubsection{Zero-Shot Evaluation}

We report the result of base models for zero-shot evaluation in Table \ref{tab:2}. The tasks are listed below. After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve superior performance compared with state-of-the-art (SOTA) baselines. This significant increase from the base model demonstrates the effectiveness of our fine-tuning approach. The improved performance is particularly notable on complex reasoning tasks like PIQA, where the score increased from 78.07\% to 82.24\%, matching or exceeding several leading models.
Consequently, our models emerge as an excellent candidate for real-world applications. 
\begin{itemize}
\item AI2 Reasoning Challenge (0-shot)
\item AI2 Reasoning Easy (0-shot)
\item HellaSwag (0-shot)
\item PIQA (0-shot)
\item Winogrande (0-shot)
\end{itemize}

\scalebox{1.04}{
\begin{threeparttable}[t]
\caption{Performance comparison for various models in zero-shot evaluation.}
\begin{tabular}{c|ccccc|c}
\hline
Models                & HellaSwag & WinoGrade & PIQA  & ARC-E & ARC-C & Ave \\ \hline
Mistral - 7B       & 80.39     & 73.4      & 82.15 & 78.28 & 52.22 & 73.29   \\
LLaMA 2 - 7B          & 75.99     & 69.06     & 79.11 & 74.54 & 46.42 & 69.02   \\
LLaMA 2 - 13B         & 79.37     & 72.22     & 80.52 & 77.4  & 49.06 & 71.71   \\
LLaMA 3.1 - 8B        & 78.92     & 74.19     & 81.12 & 81.06 & 53.67 & 73.79   \\
gemma - 7b            & 80.45     & 73.72     & 80.9  & 79.97 & 54.1  & 73.83   \\
Qwen v2 - 7B          & 78.9      & 72.38     & 79.98 & 74.71 & 50.09 & 71.21   \\
internlm2.5 - 7b      & 79.14     & 77.9      & 80.52 & 76.16 & 51.37 & 73.02   \\
Baichuan2 - 7B        & 72.25     & 67.17     & 77.26 & 72.98 & 42.15 & 66.36   \\
Yi-1.5-9B             & 77.86     & 73.01     & 80.74 & 79.04 & 55.03 & 73.14   \\
deepseek - 7B         & 76.13     & 69.77     & 79.76 & 71.04 & 44.8  & 68.3    \\ \hline
Moxin - 7B - original & 72.06     & 66.31     & 78.07 & 71.47 & 48.15 & 67.21   \\
Moxin - 7B - finetune & 80.03     & 75.17     & 82.24 & 81.12 & 58.64 & 75.44   \\ \hline
\end{tabular}
\label{tab:2}
\end{threeparttable}}


\subsubsection{Few-Shot Evaluation}

Table \ref{tab:1} presents our zero-shot evaluation results across multiple benchmark tasks. The tasks and their few-show settings are listed below. Thanks to its rigorous and high-quality training corpus, our model demonstrates a remarkable competitive edge in tasks that involve language understanding and knowledge application. Our Moxin-7B-original achieves superior performance than LLaMA2-7B in this scenario. After training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve competitive performance compared with  SOTA baselines.


Consequently, our models emerge as an excellent choice for a multitude of real-world applications where the reliance on robust language comprehension and extensive knowledge is paramount.
\begin{itemize}
\item AI2 Reasoning Challenge (25-shot)
\item HellaSwag (10-shot)
\item MMLU (5-shot)
\item Winogrande (5-shot)
\end{itemize}




\scalebox{1.2}{
\begin{threeparttable}[t]
\caption{Performance comparison for various models in few-shot evaluation.}
\begin{tabular}{c|cccc|c}
\hline
model                  & ARC-C   & hellaswag & mmlu  & WinoGrade & Ave   \\ \hline
Mistral - 7B      & 57.59 & 83.25     & 62.42 & 78.77     & 70.51 \\
LLaMA 3.1 - 8B         & 54.61 & 81.95     & 65.16 & 77.35     & 69.77 \\
LLaMA 3 - 8B           & 55.46 & 82.09     & 65.29 & 77.82     & 70.17 \\
LLaMA 2 - 7B           & 49.74 & 78.94     & 45.89 & 74.27     & 62.21 \\
Qwen 2 - 7B            & 57.68 & 80.76     & 70.42 & 77.43     & 71.57 \\
gemma - 7B             & 56.48 & 82.31     & 63.02 & 78.3      & 70.03 \\
internlm2.5 - 7B       & 54.78 & 79.7      & 68.17 & 80.9      & 70.89 \\
Baichuan2 - 7B         & 47.87 & 73.89     & 54.13 & 70.8      & 61.67 \\
Yi-1.5-9B              & 58.36 & 80.36     & 69.54 & 77.53     & 71.48 \\ \hline
Moxin - 7B - original  & 53.75 & 75.46     & 59.43 & 70.32     & 64.74 \\
Moxin - 7B - finetuned & 59.47 & 83.08     & 60.97 & 78.69     & 70.55 \\ \hline
\end{tabular}
\label{tab:1}
\end{threeparttable}}





\subsection{Alignment Evaluation}  

\begin{wraptable}{r}{5.2cm}
\vspace{-5pt}\caption{Performance  for various chat models.}\label{tab:4}
\begin{tabular}{c|c}
\hline
Model                   & MTbench       \\ \hline
\textbf{Moxin Chat 7B} & \textbf{6.42} \\
Llama 2 Chat 13B         & 6.65          \\
Vicuna 13B              & 6.57          \\
Llama 2 Chat 7B          & 6.27          \\
Vicuna 7B               & 6.17          \\
Alpaca 13B              & 4.53          \\
\hline
\end{tabular}
\end{wraptable}
We evaluate the alignment performance on MTBench \cite{zheng2023judging}. 
It is a two-round conversation dataset with 80 questions. It covers eight dimensions (reasoning, roleplay, math, coding, writing, humanities, STEM, and information extraction) with 10 questions for each dimension. The model needs to answer the first question and then refine its previous response following additional specific instructions.  We use GPT-4 as a judge model to provide scores  (between 1-10) for the quality of responses. Our Moxin-7B-chat achieves superior performance on MTbench compared with baselines, as shown in Table~\ref{tab:4}. 


\subsection{Generation Examples}  
We demonstrate some text generation examples from our Moxin Chat 7B and Llama2 Chat 7B in Table \ref{tab:E1} and \ref{tab:E2}.


\begin{center}
\begin{threeparttable}[]

\scalebox{0.7}{
\begin{tabular}{l|l}
\hline
               & Example 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\ \hline
Moxin Chat 7B  & \begin{tabular}[c]{@{}l@{}}Can you explain the concept of regularization in machine learning? \\ \\ Regularization is a technique used in machine learning to prevent overfitting by introducing a penalty \\ term in the loss function. The idea behind regularization is to encourage simpler models that are easier to \\ interpret and generalize better to new data. \\ \\ There are two main types of regularization: \\ 1. L1 regularization: also known as Lasso regularization, this method adds a penalty term to the loss \\ function that is proportional to the absolute value of the coefficients in the model. This encourages \\ sparsity, meaning that some coefficients may be exactly zero, which simplifies the model and makes it \\ easier to interpret. \\ 2. L2 regularization: also known as Ridge regularization, this method adds a penalty term to the loss \\ function that is proportional to the square of the coefficients in the model. This encourages shrinkage, \\ meaning that the coefficients are pulled closer to zero, which simplifies the model and reduces the \\ variance of the model estimates. \\ \\ Regularization can be used in various machine learning models, including linear regression, logistic \\ regression, and neural networks. It is often used in conjunction with other techniques such as cross-\\ validation to find the optimal hyperparameters that balance the bias-variance tradeoff and prevent \\ overfitting.\end{tabular}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\ \hline
Llama2 Chat 7B & \begin{tabular}[c]{@{}l@{}}Can you explain the concept of regularization in machine learning? \\ \\ Regularization is a technique used in machine learning to prevent overfitting. It involves adding a \\ penalty term to the loss function to discourage large values of the model's weights. \\ \\ There are several types of regularization, including: \\ 1. L1 regularization (Lasso): This adds a penalty term to the loss function that is proportional to the \\ absolute value of the model's weights. This encourages the model to have smaller weights, which can \\ help prevent overfitting. \\ 2. L2 regularization (Ridge): This adds a penalty term to the loss function that is proportional to the \\ square of the model's weights. This also encourages the model to have smaller weights, but it can lead \\ to some weights being set to zero, which can result in a simpler model. \\ 3. Dropout regularization: This is a type of regularization that is applied during training. It involves \\ randomly setting a fraction of the model's neurons to zero during each training iteration. This helps to \\ prevent overfitting by making the model less reliant on any single neuron. 4. Early stopping: This is a \\ type of regularization that is applied during training. It involves monitoring the validation loss during \\ training and stopping the training process when the validation loss stops improving. This helps to prevent \\ overfitting by stopping the training process before the model has a chance to fit the noise in the training \\ data. \\ \\ Regularization can be applied to different layers of a neural network, such as the input layer, hidden \\ layers, or output layer. It can also be applied to different parts of a model, such as the weights, biases, or \\ activation functions. The main idea behind regularization is to add a penalty term to the loss function that \\ discourages large values of the model's weights. This helps to prevent overfitting by encouraging the \\ model to find a simpler solution that generalizes better to new data. \\ \\ Regularization can be used in conjunction with other techniques, such as cross-validation and grid search, \\ to improve the performance of a machine learning model. In summary, regularization is a technique used \\ in machine learning to prevent overfitting by adding a penalty term to the loss function. There are several \\ types of regularization, including L1, L2, dropout, and early stopping, and it can be applied to different \\ layers and parts of a model. Regularization helps to find a simpler solution that generalizes better to new \\ data and can be used in conjunction with other techniques to improve the performance of a machine \\ learning model.\end{tabular} \\ \hline
\end{tabular}}
\caption{Generation Examples.} \label{tab:E1}
\end{threeparttable}
\end{center}





\begin{center}
\begin{threeparttable}[t]
\scalebox{0.7}{
\begin{tabular}{l|l}
\hline                                                               & Example 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\ \hline
Moxin Chat 7B  & \begin{tabular}[c]{@{}l@{}}Can you explain the concept of normalization in machine learning? \\ \\ Normalization is a technique used to scale the data before using it for training a machine learning model. \\ It is used to make the data more uniform, and to prevent it from being skewed by large or small values. \\ \\ There are several ways to normalize data, including min-max scaling, z-score normalization, and \\ logarithmic scaling. In min-max scaling, the values are scaled to a range between 0 and 1, by dividing \\ them by the maximum value in the dataset. In z-score normalization, the values are scaled to have a mean \\ of 0 and a standard deviation of 1, by subtracting the mean from each value and dividing by the standard \\ deviation. In logarithmic scaling, the values are transformed using the logarithm function, which can be \\ useful for normalizing data with a wide range of values. \\ \\ Normalization is an important step in preparing data for machine learning, as it can help to improve the \\ performance of the model by making the data more uniform and easier to analyze. It can also help to \\ prevent overfitting and improve generalization.\end{tabular}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\ \hline
Llama2 Chat 7B & \begin{tabular}[c]{@{}l@{}}Can you explain the concept of normalization in machine learning? \\ \\ Normalization is a technique used in machine learning to transform input data into a range of values that \\ are suitable for use in a model. The goal of normalization is to ensure that the data is on the same scale, so \\ that the model can learn meaningful relationships between the features. \\ \\ There are several types of normalization techniques, including: \\ 1. Min-max normalization: This involves scaling the data so that it falls within a specific range, usually \\ between 0 and 1. \\ 2. Standardization: This involves subtracting the mean and dividing by the standard deviation of the data, \\ so that the data has a mean of 0 and a standard deviation of 1. \\ 3. Z-score normalization: This involves subtracting the mean and dividing by the standard deviation of \\ the data, so that the data has a mean of 0 and a standard deviation of 1. \\ 4. Log transformation: This involves taking the logarithm of the data, which can help to reduce the effect \\ of extreme values in the data. \\ 5. Square root transformation: This involves taking the square root of the data, which can help to reduce \\ the effect of outliers in the data. \\ \\ Normalization is an important preprocessing step in machine learning, as it can help to improve the \\ performance of the model by reducing the effect of irrelevant features and improving the interpretability \\ of the results. \\ \\ Here are some key points to keep in mind when normalizing data for machine learning: \\ 1. Normalization can improve the performance of the model by reducing the effect of irrelevant features \\ and improving the interpretability of the results. \\ 2. There are several types of normalization techniques, including min-max normalization, standardization, \\ z-score normalization, log transformation, and square root transformation. \\ 3. The choice of normalization technique will depend on the nature of the data and the specific \\ requirements of the model. \\ 4. Normalization can be applied to both continuous and categorical data. \\ 5. It is important to normalize the data before training the model, as normalization can improve the \\ convergence of the model and reduce the risk of overfitting. \\ 6. Normalization can also be used to improve the interpretability of the results by reducing the effect of \\ irrelevant features and improving the scalability of the model. \\ 7. It is important to carefully evaluate the performance of the normalized data and consider the trade-offs \\ between different normalization techniques. \\ 8. Normalization can be applied to different features of the data, such as the input features, the target \\ variable, or the output features. \\ 9. Normalization can be used in combination with other preprocessing techniques, such as feature scaling, \\ feature selection, and data augmentation. \\ 10. Normalization is a powerful tool for improving the performance of machine learning models, but it is \\ important to carefully evaluate the performance of the normalized data and consider the trade-offs \\ between different normalization techniques.\end{tabular} \\ \hline
\end{tabular}}
\caption{Generation Examples.}
\label{tab:E2}
\end{threeparttable}
\end{center}

