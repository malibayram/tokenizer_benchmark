\section{Related Work}

Tokenization plays a fundamental role in natural language processing (NLP), directly influencing the performance, efficiency, and accuracy of large language models (LLMs). Recent research has explored various tokenization strategies and their downstream impacts, aiming to balance linguistic fidelity, computational efficiency, and model scalability.

The \textit{Arabic Tokenizers Leaderboard} \cite{rashad_arabic_nodate} benchmarks tokenizers for Arabic, using datasets such as \texttt{rasaif-translations} and \texttt{Moroccan Arabic Wikipedia}, highlighting the unique challenges posed by Arabic's diverse dialects and orthographic complexity. Tools like \textit{AraNizer} \cite{koubaa_githubcomriotu-labaranizer_2024} leverage subword-based techniques, such as Byte Pair Encoding (BPE) and SentencePiece, to better capture the morphological nuances of Arabic and enhance downstream performance.

Similarly, the \textit{NbAiLab Tokenizer Benchmark} \cite{rosa_nbailabtokenizer-benchmark_2024} evaluates tokenization strategies for Scandinavian languages, emphasizing the critical need for language-specific adaptations in multilingual contexts. For German, Diewald et al. \cite{diewald_tokenizing_2022} assessed tokenizers like \texttt{KorAP-Tokenizer} and \texttt{SoMaJo}, achieving high accuracy in token boundary detection while ensuring computational efficiency for large-scale corpora.

A significant contribution to multilingual tokenization comes from the EuroLLM team, which emphasizes the importance of designing tokenizers with large vocabularies to support diverse linguistic structures \cite{martins_eurollm_2024}. By employing a Byte Pair Encoding (BPE) tokenizer with byte fallback and a vocabulary of 128,000 pieces, EuroLLM achieves a balance between low fertility (tokens per word) and parameter efficiency. Their findings indicate that vocabulary size is a critical factor in determining a tokenizer’s ability to efficiently process multiple languages, including European and non-European ones. EuroLLM's comparison of fertility metrics across tokenizers, such as those from Mistral, LLaMA-3, and Gemma, further underscores the trade-offs between large vocabularies and computational cost.

Efficiency advancements have also been demonstrated in GitHub's faster BPE implementation \cite{neubeck_so_2024}, which significantly improves scalability for tasks requiring billions of tokens. This aligns with EuroLLM’s approach of optimizing tokenization to enhance downstream task performance while maintaining computational efficiency.

Rust et al. \cite{rust_how_2021} highlight the effectiveness of monolingual tokenizers tailored to specific languages, showing notable downstream improvements for morphologically rich languages. Similarly, Lin et al. \cite{lin_not_nodate} propose Selective Language Modeling (SLM), which assigns utility scores to tokens and selectively trains on high-utility tokens, reducing noise and enhancing training efficiency. This approach is particularly relevant for languages like Turkish, where preserving meaningful tokens is essential for capturing linguistic richness.

The studies discussed collectively emphasize the necessity of tokenization strategies that balance linguistic integrity, computational efficiency, and downstream performance. Building on these advancements, this study evaluates tokenizers for Turkish, employing metrics such as token purity, vocabulary size, and processing speed. By integrating insights from multilingual projects like EuroLLM and tailoring techniques for morphologically rich languages, this work advances the understanding and optimization of tokenization for diverse linguistic contexts.