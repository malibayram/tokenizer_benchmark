{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1605376"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from is_turkish import get_is_turkish\n",
    "from is_pure import get_is_pure\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/alibayram/yapay_zeka_turkce_mmlu_model_cevaplari/data/train-00000-of-00001.parquet\")\n",
    "df = df[[\"soru\", \"secenekler\"]]\n",
    "text = \"\"\n",
    "for _, row in df.iterrows():\n",
    "    text += row[\"soru\"] + \"\\n\"  \n",
    "    for secenek in row[\"secenekler\"]:\n",
    "        text += secenek + \"\\n\"\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198193"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tokenizer(text:str, tokenizer_score_map: dict):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_score_map[\"tokenizer\"])\n",
    "  tokenizer_score_map[\"vocab-size\"] = len(tokenizer.get_vocab())\n",
    "  time_start = time.time()\n",
    "\n",
    "  token_ids = tokenizer.encode(text)\n",
    "  tokenizer_score_map[\"tokens-count\"] = len(token_ids)\n",
    "\n",
    "  tokens = []\n",
    "\n",
    "  for token_id in token_ids:\n",
    "    try:\n",
    "      token = tokenizer.decode(token_id)\n",
    "      tokens.append(token)\n",
    "    except:\n",
    "      print(\"Error: \", token_id)\n",
    "\n",
    "  time_end = time.time()\n",
    "\n",
    "  tokenizer_score_map[\"time\"] = round(time_end - time_start, 4)\n",
    "  \n",
    "  tokens = set(tokens)\n",
    "  is_turkish_map = get_is_turkish(tokens)\n",
    "  tokenizer_score_map[\"unique-token-count\"] = len(is_turkish_map)\n",
    "\n",
    "  # count of true values in is_turkish_map\n",
    "  tokenizer_score_map[\"turkish-token-count\"] = sum(is_turkish_map.values())\n",
    "  tokenizer_score_map[\"turkish-token-percent\"] = round(tokenizer_score_map[\"turkish-token-count\"] / tokenizer_score_map[\"unique-token-count\"], 4)\n",
    "\n",
    "  is_pure_map = get_is_pure(\" \".join(is_turkish_map.keys()))\n",
    "  # count of true values in is_pure_map\n",
    "  tokenizer_score_map[\"pure-token-count\"] = sum(is_pure_map.values())\n",
    "  tokenizer_score_map[\"pure-token-percent\"] = round(tokenizer_score_map[\"pure-token-count\"] / tokenizer_score_map[\"unique-token-count\"], 4)\n",
    "  \n",
    "\n",
    "  return tokenizer_score_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [\n",
    "  \"google/gemma-2-9b\",\n",
    "  \"alibayram/tr_tokenizer\",\n",
    "  \"AhmetSemih/tr_tokenizer\",\n",
    "  \"aliarda/turkish_tokenizer_256k\",\n",
    "  \"aliarda/turkish_tokenizer\",\n",
    "  \"meta-llama/Llama-3.2-3B\",\n",
    "  \"utter-project/EuroLLM-9B-Instruct\",\n",
    "  \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "  \"CohereForAI/aya-expanse-8b\",\n",
    "  \"openai-community/gpt2\",\n",
    "  \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "  \"microsoft/Phi-3.5-mini-instruct\",\n",
    "  \"Trendyol/Trendyol-LLM-8b-chat-v2.0\",\n",
    "  \"ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_score_maps = []\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "  tokenizer_score_map = {\n",
    "    \"tokenizer\": tokenizer\n",
    "  }\n",
    "  tokenizer_score_map = score_tokenizer(text, tokenizer_score_map)\n",
    "  tokenizer_score_maps.append(tokenizer_score_map)\n",
    "  print(tokenizer_score_map)\n",
    "\n",
    "df = pd.DataFrame(tokenizer_score_maps)\n",
    "df.to_csv(\"tokenizer_score_maps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vocab-size</th>\n",
       "      <th>tokens-count</th>\n",
       "      <th>time</th>\n",
       "      <th>unique-token-count</th>\n",
       "      <th>turkish-token-count</th>\n",
       "      <th>turkish-token-percent</th>\n",
       "      <th>pure-token-count</th>\n",
       "      <th>pure-token-percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/gemma-2-9b</td>\n",
       "      <td>256000</td>\n",
       "      <td>497015</td>\n",
       "      <td>2.9500</td>\n",
       "      <td>6383</td>\n",
       "      <td>3104</td>\n",
       "      <td>0.4863</td>\n",
       "      <td>2365</td>\n",
       "      <td>0.3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alibayram/tr_tokenizer</td>\n",
       "      <td>30158</td>\n",
       "      <td>476556</td>\n",
       "      <td>2.4231</td>\n",
       "      <td>11531</td>\n",
       "      <td>11342</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>11055</td>\n",
       "      <td>0.9587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AhmetSemih/tr_tokenizer</td>\n",
       "      <td>59572</td>\n",
       "      <td>451883</td>\n",
       "      <td>2.4849</td>\n",
       "      <td>13370</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>13357</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aliarda/turkish_tokenizer_256k</td>\n",
       "      <td>256000</td>\n",
       "      <td>488267</td>\n",
       "      <td>2.5124</td>\n",
       "      <td>13631</td>\n",
       "      <td>13351</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>12981</td>\n",
       "      <td>0.9523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aliarda/turkish_tokenizer</td>\n",
       "      <td>58526</td>\n",
       "      <td>451936</td>\n",
       "      <td>2.3406</td>\n",
       "      <td>13268</td>\n",
       "      <td>13170</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>13256</td>\n",
       "      <td>0.9991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>128256</td>\n",
       "      <td>488535</td>\n",
       "      <td>3.1249</td>\n",
       "      <td>6823</td>\n",
       "      <td>3125</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>2109</td>\n",
       "      <td>0.3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>utter-project/EuroLLM-9B-Instruct</td>\n",
       "      <td>128000</td>\n",
       "      <td>497173</td>\n",
       "      <td>3.2019</td>\n",
       "      <td>5226</td>\n",
       "      <td>2457</td>\n",
       "      <td>0.4701</td>\n",
       "      <td>1838</td>\n",
       "      <td>0.3517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct</td>\n",
       "      <td>151665</td>\n",
       "      <td>561866</td>\n",
       "      <td>3.3150</td>\n",
       "      <td>5752</td>\n",
       "      <td>2320</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>1734</td>\n",
       "      <td>0.3015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CohereForAI/aya-expanse-8b</td>\n",
       "      <td>255029</td>\n",
       "      <td>434526</td>\n",
       "      <td>2.7651</td>\n",
       "      <td>8562</td>\n",
       "      <td>4338</td>\n",
       "      <td>0.5067</td>\n",
       "      <td>2822</td>\n",
       "      <td>0.3296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>openai-community/gpt2</td>\n",
       "      <td>50257</td>\n",
       "      <td>821139</td>\n",
       "      <td>4.3765</td>\n",
       "      <td>3454</td>\n",
       "      <td>1582</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mistralai/Mistral-Nemo-Instruct-2407</td>\n",
       "      <td>131072</td>\n",
       "      <td>534930</td>\n",
       "      <td>3.1405</td>\n",
       "      <td>4354</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>1571</td>\n",
       "      <td>0.3608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>microsoft/Phi-3.5-mini-instruct</td>\n",
       "      <td>32011</td>\n",
       "      <td>803971</td>\n",
       "      <td>4.5526</td>\n",
       "      <td>3640</td>\n",
       "      <td>1599</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>1253</td>\n",
       "      <td>0.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trendyol/Trendyol-LLM-8b-chat-v2.0</td>\n",
       "      <td>128256</td>\n",
       "      <td>488535</td>\n",
       "      <td>3.0438</td>\n",
       "      <td>6823</td>\n",
       "      <td>3125</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>2109</td>\n",
       "      <td>0.3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ytu-ce-cosmos/turkish-gpt2-large-750m-instruct...</td>\n",
       "      <td>50258</td>\n",
       "      <td>339852</td>\n",
       "      <td>2.4044</td>\n",
       "      <td>22746</td>\n",
       "      <td>17529</td>\n",
       "      <td>0.7706</td>\n",
       "      <td>6291</td>\n",
       "      <td>0.2766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenizer  vocab-size  \\\n",
       "0                                   google/gemma-2-9b      256000   \n",
       "1                              alibayram/tr_tokenizer       30158   \n",
       "2                             AhmetSemih/tr_tokenizer       59572   \n",
       "3                      aliarda/turkish_tokenizer_256k      256000   \n",
       "4                           aliarda/turkish_tokenizer       58526   \n",
       "5                             meta-llama/Llama-3.2-3B      128256   \n",
       "6                   utter-project/EuroLLM-9B-Instruct      128000   \n",
       "7                            Qwen/Qwen2.5-7B-Instruct      151665   \n",
       "8                          CohereForAI/aya-expanse-8b      255029   \n",
       "9                               openai-community/gpt2       50257   \n",
       "10               mistralai/Mistral-Nemo-Instruct-2407      131072   \n",
       "11                    microsoft/Phi-3.5-mini-instruct       32011   \n",
       "12                 Trendyol/Trendyol-LLM-8b-chat-v2.0      128256   \n",
       "13  ytu-ce-cosmos/turkish-gpt2-large-750m-instruct...       50258   \n",
       "\n",
       "    tokens-count    time  unique-token-count  turkish-token-count  \\\n",
       "0         497015  2.9500                6383                 3104   \n",
       "1         476556  2.4231               11531                11342   \n",
       "2         451883  2.4849               13370                13253   \n",
       "3         488267  2.5124               13631                13351   \n",
       "4         451936  2.3406               13268                13170   \n",
       "5         488535  3.1249                6823                 3125   \n",
       "6         497173  3.2019                5226                 2457   \n",
       "7         561866  3.3150                5752                 2320   \n",
       "8         434526  2.7651                8562                 4338   \n",
       "9         821139  4.3765                3454                 1582   \n",
       "10        534930  3.1405                4354                 1971   \n",
       "11        803971  4.5526                3640                 1599   \n",
       "12        488535  3.0438                6823                 3125   \n",
       "13        339852  2.4044               22746                17529   \n",
       "\n",
       "    turkish-token-percent  pure-token-count  pure-token-percent  \n",
       "0                  0.4863              2365              0.3705  \n",
       "1                  0.9836             11055              0.9587  \n",
       "2                  0.9912             13357              0.9990  \n",
       "3                  0.9795             12981              0.9523  \n",
       "4                  0.9926             13256              0.9991  \n",
       "5                  0.4580              2109              0.3091  \n",
       "6                  0.4701              1838              0.3517  \n",
       "7                  0.4033              1734              0.3015  \n",
       "8                  0.5067              2822              0.3296  \n",
       "9                  0.4580              1119              0.3240  \n",
       "10                 0.4527              1571              0.3608  \n",
       "11                 0.4393              1253              0.3442  \n",
       "12                 0.4580              2109              0.3091  \n",
       "13                 0.7706              6291              0.2766  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Risk yönet'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "enc.decode([43519, 95938])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_openai_tokenizer(text:str, tokenizer_score_map: dict):\n",
    "\n",
    "  tokenizer_score_map[\"vocab-size\"] = enc.n_vocab\n",
    "  time_start = time.time()\n",
    "\n",
    "  token_ids = enc.encode(text)\n",
    "  tokenizer_score_map[\"tokens-count\"] = len(token_ids)\n",
    "\n",
    "  tokens = []\n",
    "\n",
    "  for token_id in token_ids:\n",
    "    try:\n",
    "      token = enc.decode([token_id])\n",
    "      tokens.append(token)\n",
    "    except:\n",
    "      print(\"Error: \", token_id)\n",
    "\n",
    "  time_end = time.time()\n",
    "\n",
    "  tokenizer_score_map[\"time\"] = round(time_end - time_start, 4)\n",
    "  \n",
    "  tokens = set(tokens)\n",
    "  is_turkish_map = get_is_turkish(tokens)\n",
    "  tokenizer_score_map[\"unique-token-count\"] = len(is_turkish_map)\n",
    "\n",
    "  # count of true values in is_turkish_map\n",
    "  tokenizer_score_map[\"turkish-token-count\"] = sum(is_turkish_map.values())\n",
    "  tokenizer_score_map[\"turkish-token-percent\"] = round(tokenizer_score_map[\"turkish-token-count\"] / tokenizer_score_map[\"unique-token-count\"], 4)\n",
    "\n",
    "  is_pure_map = get_is_pure(\" \".join(is_turkish_map.keys()))\n",
    "  # count of true values in is_pure_map\n",
    "  tokenizer_score_map[\"pure-token-count\"] = sum(is_pure_map.values())\n",
    "  tokenizer_score_map[\"pure-token-percent\"] = round(tokenizer_score_map[\"pure-token-count\"] / tokenizer_score_map[\"unique-token-count\"], 4)\n",
    "  \n",
    "\n",
    "  return tokenizer_score_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/pip_system_certs/wrapt_requests.py:71: UserWarning: Failed to patch SSL settings for unverified requests (unsupported version of urllib3?)\n",
      "This may lead to errors when urllib3 tries to modify verify_mode.\n",
      "Please report an issue at https://gitlab.com/alelec/pip-system-certs with your\n",
      "python version included in the description\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "openai_tokenizer_score_map = {\n",
    "  \"tokenizer\": \"openai/o200k_base-gpt-4o\"\n",
    "}\n",
    "\n",
    "openai_tokenizer_score_map = score_openai_tokenizer(text, openai_tokenizer_score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vocab-size</th>\n",
       "      <th>tokens-count</th>\n",
       "      <th>time</th>\n",
       "      <th>unique-token-count</th>\n",
       "      <th>turkish-token-count</th>\n",
       "      <th>turkish-token-percent</th>\n",
       "      <th>pure-token-count</th>\n",
       "      <th>pure-token-percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/gemma-2-9b</td>\n",
       "      <td>256000</td>\n",
       "      <td>497015</td>\n",
       "      <td>2.9500</td>\n",
       "      <td>6383</td>\n",
       "      <td>3104</td>\n",
       "      <td>0.4863</td>\n",
       "      <td>2365</td>\n",
       "      <td>0.3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alibayram/tr_tokenizer</td>\n",
       "      <td>30158</td>\n",
       "      <td>476556</td>\n",
       "      <td>2.4231</td>\n",
       "      <td>11531</td>\n",
       "      <td>11342</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>11055</td>\n",
       "      <td>0.9587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AhmetSemih/tr_tokenizer</td>\n",
       "      <td>59572</td>\n",
       "      <td>451883</td>\n",
       "      <td>2.4849</td>\n",
       "      <td>13370</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>13357</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aliarda/turkish_tokenizer_256k</td>\n",
       "      <td>256000</td>\n",
       "      <td>488267</td>\n",
       "      <td>2.5124</td>\n",
       "      <td>13631</td>\n",
       "      <td>13351</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>12981</td>\n",
       "      <td>0.9523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aliarda/turkish_tokenizer</td>\n",
       "      <td>58526</td>\n",
       "      <td>451936</td>\n",
       "      <td>2.3406</td>\n",
       "      <td>13268</td>\n",
       "      <td>13170</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>13256</td>\n",
       "      <td>0.9991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>128256</td>\n",
       "      <td>488535</td>\n",
       "      <td>3.1249</td>\n",
       "      <td>6823</td>\n",
       "      <td>3125</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>2109</td>\n",
       "      <td>0.3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>utter-project/EuroLLM-9B-Instruct</td>\n",
       "      <td>128000</td>\n",
       "      <td>497173</td>\n",
       "      <td>3.2019</td>\n",
       "      <td>5226</td>\n",
       "      <td>2457</td>\n",
       "      <td>0.4701</td>\n",
       "      <td>1838</td>\n",
       "      <td>0.3517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct</td>\n",
       "      <td>151665</td>\n",
       "      <td>561866</td>\n",
       "      <td>3.3150</td>\n",
       "      <td>5752</td>\n",
       "      <td>2320</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>1734</td>\n",
       "      <td>0.3015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CohereForAI/aya-expanse-8b</td>\n",
       "      <td>255029</td>\n",
       "      <td>434526</td>\n",
       "      <td>2.7651</td>\n",
       "      <td>8562</td>\n",
       "      <td>4338</td>\n",
       "      <td>0.5067</td>\n",
       "      <td>2822</td>\n",
       "      <td>0.3296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>openai-community/gpt2</td>\n",
       "      <td>50257</td>\n",
       "      <td>821139</td>\n",
       "      <td>4.3765</td>\n",
       "      <td>3454</td>\n",
       "      <td>1582</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mistralai/Mistral-Nemo-Instruct-2407</td>\n",
       "      <td>131072</td>\n",
       "      <td>534930</td>\n",
       "      <td>3.1405</td>\n",
       "      <td>4354</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>1571</td>\n",
       "      <td>0.3608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>microsoft/Phi-3.5-mini-instruct</td>\n",
       "      <td>32011</td>\n",
       "      <td>803971</td>\n",
       "      <td>4.5526</td>\n",
       "      <td>3640</td>\n",
       "      <td>1599</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>1253</td>\n",
       "      <td>0.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trendyol/Trendyol-LLM-8b-chat-v2.0</td>\n",
       "      <td>128256</td>\n",
       "      <td>488535</td>\n",
       "      <td>3.0438</td>\n",
       "      <td>6823</td>\n",
       "      <td>3125</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>2109</td>\n",
       "      <td>0.3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ytu-ce-cosmos/turkish-gpt2-large-750m-instruct...</td>\n",
       "      <td>50258</td>\n",
       "      <td>339852</td>\n",
       "      <td>2.4044</td>\n",
       "      <td>22746</td>\n",
       "      <td>17529</td>\n",
       "      <td>0.7706</td>\n",
       "      <td>6291</td>\n",
       "      <td>0.2766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>openai/o200k_base-gpt-4o</td>\n",
       "      <td>200019</td>\n",
       "      <td>491137</td>\n",
       "      <td>0.5147</td>\n",
       "      <td>7615</td>\n",
       "      <td>3209</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>2184</td>\n",
       "      <td>0.2868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenizer  vocab-size  \\\n",
       "0                                   google/gemma-2-9b      256000   \n",
       "1                              alibayram/tr_tokenizer       30158   \n",
       "2                             AhmetSemih/tr_tokenizer       59572   \n",
       "3                      aliarda/turkish_tokenizer_256k      256000   \n",
       "4                           aliarda/turkish_tokenizer       58526   \n",
       "5                             meta-llama/Llama-3.2-3B      128256   \n",
       "6                   utter-project/EuroLLM-9B-Instruct      128000   \n",
       "7                            Qwen/Qwen2.5-7B-Instruct      151665   \n",
       "8                          CohereForAI/aya-expanse-8b      255029   \n",
       "9                               openai-community/gpt2       50257   \n",
       "10               mistralai/Mistral-Nemo-Instruct-2407      131072   \n",
       "11                    microsoft/Phi-3.5-mini-instruct       32011   \n",
       "12                 Trendyol/Trendyol-LLM-8b-chat-v2.0      128256   \n",
       "13  ytu-ce-cosmos/turkish-gpt2-large-750m-instruct...       50258   \n",
       "14                           openai/o200k_base-gpt-4o      200019   \n",
       "\n",
       "    tokens-count    time  unique-token-count  turkish-token-count  \\\n",
       "0         497015  2.9500                6383                 3104   \n",
       "1         476556  2.4231               11531                11342   \n",
       "2         451883  2.4849               13370                13253   \n",
       "3         488267  2.5124               13631                13351   \n",
       "4         451936  2.3406               13268                13170   \n",
       "5         488535  3.1249                6823                 3125   \n",
       "6         497173  3.2019                5226                 2457   \n",
       "7         561866  3.3150                5752                 2320   \n",
       "8         434526  2.7651                8562                 4338   \n",
       "9         821139  4.3765                3454                 1582   \n",
       "10        534930  3.1405                4354                 1971   \n",
       "11        803971  4.5526                3640                 1599   \n",
       "12        488535  3.0438                6823                 3125   \n",
       "13        339852  2.4044               22746                17529   \n",
       "14        491137  0.5147                7615                 3209   \n",
       "\n",
       "    turkish-token-percent  pure-token-count  pure-token-percent  \n",
       "0                  0.4863              2365              0.3705  \n",
       "1                  0.9836             11055              0.9587  \n",
       "2                  0.9912             13357              0.9990  \n",
       "3                  0.9795             12981              0.9523  \n",
       "4                  0.9926             13256              0.9991  \n",
       "5                  0.4580              2109              0.3091  \n",
       "6                  0.4701              1838              0.3517  \n",
       "7                  0.4033              1734              0.3015  \n",
       "8                  0.5067              2822              0.3296  \n",
       "9                  0.4580              1119              0.3240  \n",
       "10                 0.4527              1571              0.3608  \n",
       "11                 0.4393              1253              0.3442  \n",
       "12                 0.4580              2109              0.3091  \n",
       "13                 0.7706              6291              0.2766  \n",
       "14                 0.4214              2184              0.2868  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tokenizer_score_maps.csv\")\n",
    "df = pd.concat([df, pd.DataFrame([openai_tokenizer_score_map])])\n",
    "df.to_csv(\"tokenizer_score_maps.csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
